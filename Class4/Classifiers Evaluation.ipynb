{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building and Assessing ML Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import needed modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Downloads\n",
    "using CSV\n",
    "using MLJ\n",
    "using DataFrames\n",
    "using GLM\n",
    "using ROCCurves\n",
    "using FreqTables\n",
    "using StatsPlots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading, pre-processing and splitting into train and validation subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading & pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/australian/australian.dat\"\n",
    "Downloads.download(url, \"australian.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CSV.read(\"australian.csv\", DataFrame, delim=' ';\n",
    "                header = append!([string(\"V\", i) for i in 0:13], [\"class\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.V3 = ifelse.(dataset.V3 .== 1, 0, 1)\n",
    "dataset.V11 = ifelse.(dataset.V11 .== 1, 0, 1)\n",
    "dataset.V13 = log.(dataset.V13) \n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_fraction = 0.6\n",
    "train, test = partition(eachindex(dataset.class), training_fraction, shuffle=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[train,:]\n",
    "test_dataset = dataset[test,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size.([train_dataset, test_dataset])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building logistic regression model\n",
    "[Logistic Regression](https://en.wikipedia.org/wiki/Logistic_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic(x) = 1 / (1 + exp(-x))\n",
    "x = collect(-6:.1:6)\n",
    "plot(x, logistic.(x), legend=false)\n",
    "vline!([0], line=:dash, color=:grey)\n",
    "hline!([1/2], line=:dash, color=:grey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_log_reg_fit = glm(Term(:class) ~ sum(Term.(Symbol.(names(dataset[:, Not(:class)])))),\n",
    "                        train_dataset, Binomial())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef(model_log_reg_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = GLM.predict(model_log_reg_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = GLM.predict(model_log_reg_fit, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal_pred = test_dataset.class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_pred = rand(length(test_dataset.class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Assessing model performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive analysis - confusion matrix and related metrics\n",
    "\n",
    "‚ùó Remember class indicator (0, 1,...) and actual or predicted values may be switched in confusion matrix\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/712/1*Z54JgbS4DUwWSknhDCvNTQ.png\" width=400>\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1780/1*LQ1YMKBlbDhH9K6Ujz8QTw.jpeg\"  width=400>\n",
    "\n",
    "**Performance measures derived from confusion matrix:**\n",
    "\n",
    "- Accuracy - percentage of correct predictions\n",
    "\n",
    "`ACC = (TP + TN)/(TP + FP + TN + FN)`\n",
    "\n",
    "- Precision - percentage of positive predictions which were actually correct\n",
    "\n",
    "`PREC = TP / (TP + FP)`\n",
    "\n",
    "-  Recall - what percentage of actual positives were predicted correctly\n",
    " (Recall = Sensitivity = Hit rate = True Positive Rate (TPR))\n",
    " \n",
    "`REC = TP / (TP + FN)`\n",
    "\n",
    "- Specificity - what percentage of actual negatives were predicted correctly (Specificity = True Negative Rate)\n",
    "\n",
    "`TNR = TN / (TN + FP)`\n",
    "\n",
    "- F1 Score - traditional F-measure or balanced F-score (F1 score) is the harmonic mean of precision and recall\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/1bf179c30b00db201ce1895d88fe2915d58e6bfd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating confusion matrices for prediction on train and test data as well as random and wizard models with a 0.5 cut-off threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat_train = freqtable(train_pred .> 0.5, train_dataset.class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat_test = freqtable(test_pred .> 0.5, test_dataset.class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat_ideal = freqtable(ideal_pred .> 0.5, test_dataset.class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat_random = freqtable(random_pred .> 0.5, test_dataset.class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing a function to calculate accuracy, precision, recall and f1-score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function quality_report(mat::AbstractMatrix)\n",
    "    acc = (mat[1,1] + mat[2,2]) / sum(mat)\n",
    "    prec = mat[2,2] / sum(mat[2,:])\n",
    "    rec = mat[2,2] / sum(mat[:,2])\n",
    "    f1 = 2 * prec * rec / (prec + rec)\n",
    "    println(\"----Classification quality report----\")\n",
    "    println(\"Accuracy: \", round(acc*100,digits=2), \"%\")\n",
    "    println(\"Precision: \", round(prec*100,digits=2), \"%\")\n",
    "    println(\"Recall: \", round(rec*100,digits=2), \"%\")\n",
    "    println(\"F1-score: \", round(f1*100,digits=2), \"%\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the reports for both datasets, as well as wizard and random models. Results are quite close similar to what we have seen on ROC curves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"Train set\")\n",
    "quality_report(conf_mat_train)\n",
    "println(\"\\nTest set\")\n",
    "quality_report(conf_mat_test)\n",
    "println(\"\\nWizard model:\")\n",
    "quality_report(conf_mat_ideal)\n",
    "println(\"\\nRandom model:\")\n",
    "quality_report(conf_mat_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual analysis of a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating and plotting ROC curves for both training and test datasets as well as for wizard and random models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fpr, train_tpr = ROCCurves.roc(train_pred, train_dataset.class)\n",
    "test_fpr, test_tpr = ROCCurves.roc(test_pred, test_dataset.class)\n",
    "ideal_fpr, ideal_tpr = ROCCurves.roc(ideal_pred, test_dataset.class)\n",
    "random_fpr, random_tpr = ROCCurves.roc(random_pred, dataset.class[test])\n",
    "\n",
    "plot(test_fpr, test_tpr, label=\"Test\", xlabel=\"False Positive Rate (FPR)\",\n",
    "    ylabel=\"True Positive Rate (TPR)\",\n",
    "    title =\"Receiver Operating Characteristic (ROC) curve\", linewidth=2, legend=:bottomright)\n",
    "plot!(train_fpr, train_tpr, label=\"Train\", linewidth=2)\n",
    "plot!(ideal_fpr, ideal_tpr, label=\"Wizard\", linewidth=2)\n",
    "plot!(random_fpr, random_tpr, label=\"Random\", linewidth=2)\n",
    "Plots.abline!(1, 0, line=:dash, label = \"TPR=FPR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideal model has ROC composed of 1-point at $(0, 1)$ resulting in perfect identification of $Y=1$ while making no errors. The better model, the closer its ROC is to this ideal point of $(0,1)$ resulting also in higher area the curve, which is numerical measurement of model performance presented below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC - Area Under Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating AUC for training, test data and wizard and random models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"AUC metric on train dataset is equal to: \", auc_roc(train_fpr, train_tpr))\n",
    "println(\"AUC metric on test dataset is equal to: \", auc_roc(test_fpr, test_tpr))\n",
    "println(\"AUC metric of wizard model is equal to: \", auc_roc(ideal_fpr, ideal_tpr))\n",
    "println(\"AUC metric of random model is equal to: \", auc_roc(random_fpr, random_tpr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- discrepancy beetween train and test AUC measurement is the sign of overfitting\n",
    "- wizard model would have AUC equal to 1 and random model around 0.5\n",
    "- simulating ROC for random model with the same number of observation as test set has, enables to understand the sampling error of test set ROC "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gain chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rpp = collect(0:length(test_tpr)-1)./(length(test_tpr)-1)\n",
    "train_rpp = collect(0:length(train_tpr)-1)./(length(train_tpr)-1)\n",
    "\n",
    "plot(test_rpp, test_tpr, label=\"Test\", xlabel=\"Rate of Positive Predictions (RPP)\",\n",
    "     ylabel=\"True Positive Rate (TPR)\", title= \"Gain chart\", linewidth=2, legend=:bottomright)\n",
    "plot!(train_rpp, train_tpr, label=\"Train\", linewidth=2)\n",
    "plot!(test_rpp, ideal_tpr, label=\"Wizard\", linewidth=2)\n",
    "plot!(test_rpp, random_tpr, label=\"Random\", linewidth=2)\n",
    "vline!([mean(dataset.class[test])] ,line=:dash, label = \"P(Y=1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- wizard model will increase linearly, so it achieves maximum of $TPR = 1$ for $RPP = P(Y=1)$\n",
    "- random model gain chart is around 45-degree line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lift chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(test_rpp, test_tpr ./ test_rpp, label=\"Test\", xlabel=\"Rate of Positive Predictions (RPP)\",\n",
    "     ylabel=\"LIFT = TPR / RPP\", title= \"Lift chart\", linewidth=2, legend=:topright,\n",
    "     ylim = (1, 0.25+1/mean(dataset.class[test])))\n",
    "plot!(train_rpp, train_tpr ./ train_rpp, label=\"Train\", linewidth=2)\n",
    "plot!(test_rpp, ideal_tpr ./ test_rpp, label=\"Wizard\", linewidth=2)\n",
    "plot!(test_rpp, random_tpr ./ test_rpp, label=\"Random\", linewidth=2)\n",
    "Plots.abline!(0, 1/mean(dataset.class[test]),  \n",
    "     line=:dash, label = \"1 / P(Y=1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lift measures how many times model's TPR is higher in comparison to TPR of random model for a given RPP, e.g. LIFT = 2 means that models indetifies 2 times more label $Y=1$ than a random model.\n",
    "- Wizard model LIFT is equal to $ 1/P(Y=1)$ for $RPP < P(Y=1)$ and afterward decrease hiperbolically towards $LIFT =1$, which is a benchmark value for a random model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score-density plots\n",
    "Predicting labels on test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_1 = test_pred[test_dataset.class .== 1]\n",
    "test_pred_0 = test_pred[test_dataset.class .== 0];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing model's score on histogram with two series - one for each class of 'target' column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram(test_pred_1, normalize=true, bins=10, label=1)\n",
    "density!(test_pred_1, label=1, linewidth=2)\n",
    "histogram!(test_pred_0, normalize=true, bins=10, label=0, seriesalpha=0.7)\n",
    "density!(test_pred_0, label=0, linewidth=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more non-overlapping distributions the better predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wizard model - ideal predicitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal_pred_1 = ideal_pred[test_dataset.class .== 1]\n",
    "ideal_pred_0 = ideal_pred[test_dataset.class .== 0];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram(ideal_pred_1.-0.5, normalize=true, bins=10, label=1)\n",
    "density!(ideal_pred_1, label=1, bandwidth=.2, linewidth=2)\n",
    "histogram!(ideal_pred_0.-0.5, normalize=true, bins=10, label=0, seriesalpha=0.7)\n",
    "density!(ideal_pred_0, label=0, bandwidth=0.2, linewidth=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No overlapping scores between $Y=1$ and $Y=0$ is equivalent with a perfect model. Some overlapped kernel densities above results from high bandwidth hyperparameter used in kerned density estimation procedure, but empiracally there's no overlap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_pred_1 = random_pred[test_dataset.class .== 1]\n",
    "random_pred_0 = random_pred[test_dataset.class .== 0];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram(random_pred_1, normalize=true, bins=10, label=1)\n",
    "density!(random_pred_1, label=1, linewidth=2)\n",
    "histogram!(random_pred_0, normalize=true, bins=10, label=0, seriesalpha=0.7)\n",
    "density!(random_pred_0, label=0, linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both distribution overlap heavily each other and are difficult to be distinguished. This is a sign of very poor predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Preparation of this workshop has been supported by the Polish National Agency for Academic Exchange under the Strategic Partnerships programme, grant number BPI/PST/2021/1/00069/U/00001.*\n",
    "\n",
    "![SGH & NAWA](../logo.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.2",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "2ca342e073bfb3bd17a792ea861f84cf447b5e37a77900005dbb48db8b7a1ded"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
