{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5640b83",
   "metadata": {},
   "source": [
    "# Unsupervised learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe584af",
   "metadata": {},
   "source": [
    "Unsupervised learning is area of machine learning focused on detecting patterns in the data and **modelling without explicitly set labels/target variable**. In contrast, supervised learning techniques are mainly based on predicting nominal features (classification) or continuous features (regression).\n",
    "\n",
    "Main tasks in the area of unsupervised learning are:\n",
    "- **dimensionality reduction**\n",
    "- **clustering**\n",
    "- anomaly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c95b1d",
   "metadata": {},
   "source": [
    "**Dimensionality reduction** algorithms aim to represent high-dimensional input data in the output space with lower dimensionality. The approach is useful for:\n",
    "- visualization of high dimensional data\n",
    "- removing noise\n",
    "- lowering the volume of the dataset, hence improving performance of other algorithms\n",
    "- obfuscating and anonymizing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210cd314",
   "metadata": {},
   "source": [
    "**Clustering** aims to differentiate the groups within the data, usually based on the distance between the observations. It's common task for customer or product datasets - segments created based on clustering results may be used in marketing activities or as an input to supervised machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66404c21",
   "metadata": {},
   "source": [
    "Importing required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246c9bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "using MultivariateStats, RDatasets, Plots, StatsBase, LinearAlgebra, ClusterAnalysis\n",
    "using UMAP: umap\n",
    "using Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be2ea26",
   "metadata": {},
   "source": [
    "## Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d8ff7a",
   "metadata": {},
   "source": [
    "Loading a dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81aa57d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset(\"datasets\", \"USArrests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b175f37",
   "metadata": {},
   "source": [
    "Transforming a dataframe into numerical matrix structure, similarly to X matrix for predictive models composed of exogenous variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ac2d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_matrix = Matrix(data[:, 2:end])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff67fe9",
   "metadata": {},
   "source": [
    "Keeping US names in a seperate `labels` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c8cb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = Vector(data[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb9b962",
   "metadata": {},
   "source": [
    "Saving data dimensions to `obs_number` and `original_size` variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfc89b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_number, original_size = size(X_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c733b4",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "We'll use PCA (Principal Components Analysis) technique to represent high dimensional data in 2-dimensional space and plot the result.\n",
    "\n",
    "PCA is popular algorithm for dimensionality reduction based on linear algebra. For input matrix (dataset) we need to calculate eigenvectors (principal components) and eigenvalues. Eigenvectors determine directions for projection in new feature space and eigenvalues determine the mangnitude ('importance') of the vectors.\n",
    "\n",
    "We start with an quick-and-easy simulated data set to generate visualisations which helps to explain the concept of PCA. Later, we move on to the real dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb81c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = 0.9\n",
    "n=1000\n",
    "corr_mat = [1 corr;\n",
    "            corr 1]\n",
    "chol_decomp = cholesky(corr_mat).L\n",
    "cor_data=[randn(n) randn(n)]*chol_decomp'\n",
    "println(\"Correlation matrix: \", cor(cor_data))\n",
    "println(\"Standard deviations: \", std(cor_data, dims=1))\n",
    "print(\"Means: \", mean(cor_data, dims=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a54d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_PCA = fit(PCA, cor_data', maxoutdim = 2, mean=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ed3980",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(cor_data[:, 1], cor_data[:, 2], markeralpha=0.05, markercolor=:lightslateblue, rightmargin=5Plots.mm,\n",
    "        ylim=(-4, 4), xlim=(-4, 4), aspect_ratio=:equal, markersize=2, layout=(1, 2),\n",
    "        xlabel=\"X1\", ylabel=\"X2\", legend=false, title=\"Input correlated \\ndata for PCA\")\n",
    "quiver!(zeros(2), zeros(2),\n",
    "        quiver=(loadings(fitted_PCA)[1, :], loadings(fitted_PCA)[2, :]), linewidth=2,\n",
    "        c=:black)\n",
    "scatter!(predict(fitted_PCA, cor_data')'[:, 1], predict(fitted_PCA, cor_data')'[:, 2],\n",
    "        markeralpha=0.05, markercolor=:lightslateblue, subplot=2,\n",
    "        ylim=(-4, 4), xlim=(-4, 4), aspect_ratio=:equal, markersize=2,\n",
    "        xlabel=\"1st Principal Component\", ylabel=\"2nd Principal Component\",\n",
    "        legend=false, title=\"Output orthogonal \\nPrincipal Components\")\n",
    "quiver!(zeros(2), zeros(2), subplot=2,\n",
    "        quiver=(predict(fitted_PCA, loadings(fitted_PCA))'[1, :],\n",
    "                predict(fitted_PCA, loadings(fitted_PCA))'[2, :]),\n",
    "        linewidth=2, c=:black)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56f50fc",
   "metadata": {},
   "source": [
    "PCA requires a data standarization to work properly. PCA on a non-standarized data will be biased towards variables with highest variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2b5320",
   "metadata": {},
   "outputs": [],
   "source": [
    "standarized_X_matrix = Matrix(mapcols(zscore, data[:,2:end]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ac35f9",
   "metadata": {},
   "source": [
    "Fitting PCA on a transposed `standarized_X_matrix` with means equal to $0$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834d30ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_PCA = fit(PCA, standarized_X_matrix';\n",
    "                maxoutdim = original_size, mean=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da7bb13",
   "metadata": {},
   "source": [
    "#### Questions:\n",
    "- What's the interpretation of loadings?\n",
    "- Why loadings are unstandarized? what it means to be standarized here?\n",
    "- What's the interpretation of Eigenvalues?\n",
    "- Explain the 'Importance of components' table in a business language.                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4872b239",
   "metadata": {},
   "outputs": [],
   "source": [
    "standarized_loadings = LinearAlgebra.eigvecs(fitted_PCA)[:,1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f245f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "standarized_loadings = LinearAlgebra.eigvecs(fitted_PCA)[:,1:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f47556e",
   "metadata": {},
   "source": [
    "Now you can predict Principal Components (also knows as embedding). You can think of Principal Components as new features characterizing observations. Usually you'll neeed signficanlty less number of Principal Compenents than original number of columns to represent the significant share of information (variance) repsented in an original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ba9514",
   "metadata": {},
   "outputs": [],
   "source": [
    "PrincipalComponents = predict(fitted_PCA, standarized_X_matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6f91a9",
   "metadata": {},
   "source": [
    "Both `predict` and `MultivariateStats.transform` will produce Pricipal Components - you can think of those as encoding the original features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f084b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "PrincipalComponents == MultivariateStats.transform(fitted_PCA, standarized_X_matrix') #encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9305d80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PC_limit = ceil(Int, maximum(abs.(PrincipalComponents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5a92e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "UStates_visualization = [ (PrincipalComponents'[i,1], PrincipalComponents'[i,2],\n",
    "                        text(data.State[i], 6, :blue)) for i=1:obs_number ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc03815",
   "metadata": {},
   "outputs": [],
   "source": [
    "loadings_visualization = [ text(names(data)[i], 10, :orange, :right) for i=1:original_size ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b48fb7",
   "metadata": {},
   "source": [
    "So-called **biplot** is the important visualization tool of PCA. Biplot dipsplays both the **pricipal componenets score** (observations, e.g. states) and the **principal component loadings**, i.e. coefficients/weights defining each of Principal Components and enabling the interpretation of Pricipal Components also in a business langaguage. Below is the **biplot** for our PCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f61b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replication of FIGURE 12.1 on p. 502 from https://hastie.su.domains/ISLR2/ISLRv2_website.pdf\n",
    "plot( ; annotations= UStates_visualization , legend = false,\n",
    "    xlim=[-PC_limit-.5,PC_limit+.5], ylim=[-PC_limit, PC_limit],\n",
    "    xlabel = \"1st Principal Component\",\n",
    "    ylabel = \"2nd Principal Component\")\n",
    "hline!([0],line=:dash, color=:grey)\n",
    "vline!([0],line=:dash, color=:grey)\n",
    "quiver!(zeros(original_size),\n",
    "        zeros(original_size),\n",
    "        quiver=(standarized_loadings[:,1],standarized_loadings[:,2]),\n",
    "        c=:orange)\n",
    "annotate!(standarized_loadings[:,1]*1.1, 1.1*standarized_loadings[:,2], loadings_visualization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdb0f88",
   "metadata": {},
   "source": [
    "#### Questions:\n",
    "- Is the plot the same as in FIGURE 12.1?\n",
    "- If not, why is so? is it a problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0453bf8",
   "metadata": {},
   "source": [
    "How efficient is reduction? How much information, i.e. variance, is \"encoded\" in low-dimentional embedding?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c589c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance = LinearAlgebra.eigvals(fitted_PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c16ed0",
   "metadata": {},
   "source": [
    "Both `LinearAlgebra.eigvals()` and `principalvars()` generate sequence of decreasing variance explained by corresponding Principal Component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5f7a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance == principalvars(fitted_PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce59fe7",
   "metadata": {},
   "source": [
    "Why does `explained_variance` sum to the number of features? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8f0633",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(explained_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b126c9f7",
   "metadata": {},
   "source": [
    "It's more convenient also in business terms to talk about the proportion of explained Variance (the same way we prefer to speak about $R^2$ - coefficient of determination rather than Mean Squared Error) rather than absolute value of total variance being explained, so we calculate `propOfExplainedVariance`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ea4116",
   "metadata": {},
   "outputs": [],
   "source": [
    "propOfExplainedVariance = explained_variance ./ sum(explained_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81509fb3",
   "metadata": {},
   "source": [
    "Below is the replication of **FIGURE 12.3** from page 507 from [**An Introduction to Statistical Learning**](https://hastie.su.domains/ISLR2/ISLRv2_website.pdf):\n",
    "- on the left: so called  **scree plot** depicting the proportion of variance explained by each of prciipal components\n",
    "- on the right: the cumulative proportion of variance explained by principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f59760",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(1:length(explained_variance), \n",
    "    Matrix( [propOfExplainedVariance  cumsum(propOfExplainedVariance)]),\n",
    "    ylim = [0,1],marker=:circle, layout=(1,2),legend = false,\n",
    "    xlabel = \"Principal Component\", rightmargin=5Plots.mm,\n",
    "    ylabel = [\"Prop. Variance Explained\" \"Cumulative Prop. Variance Explained\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a76585",
   "metadata": {},
   "source": [
    "You might want to use your reduced number of Principal Components to predict `original_size` features and see how good your PCA is able to recover original variance. This process is called \"decoding\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55c261c",
   "metadata": {},
   "outputs": [],
   "source": [
    "recovered_original_space_obs = reconstruct(fit(PCA, standarized_X_matrix';\n",
    "                                                maxoutdim = 2, mean=0),\n",
    "                                            PrincipalComponents[1:2,:])'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caf1774",
   "metadata": {},
   "source": [
    "Let's visualize how well 2-dimensional PCA is able to reconstruct 4-dimensional original set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cc0a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr(display_type=:inline) # to adjust bottom margin in order not to overlap with titles of lower row of plot\n",
    "scatter(recovered_original_space_obs,\n",
    "        standarized_X_matrix,\n",
    "        layout = (2, 2), rightmargin=5Plots.mm,\n",
    "        ylabel = \"Predicted\",\n",
    "        xlabel = \"Original value\",\n",
    "        title=[\"Murder\" \"Assault\" \"UrbanPop\" \"Rape\"],\n",
    "        legend = false, xlim=(-2,2), ylim=(-2,2))\n",
    "Plots.abline!(1, 0, line=:dash,  subplot = 1)\n",
    "Plots.abline!(1, 0, line=:dash,  subplot = 2)\n",
    "Plots.abline!(1, 0, line=:dash,  subplot = 3)\n",
    "Plots.abline!(1, 0, line=:dash,  subplot = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f853bdd1",
   "metadata": {},
   "source": [
    "PCA exploits the correlation between features, which means that variables bring similar information to each other and this information can be represented (encoded, embedded) in Principal Components. If features were orthogonal (not correlated) to each other, the would be no sense and value-added from using PCA. Luckily, typicaly in observational (in comparison to experimental) socio-economic data there's a lot of correlated variables as you can see bellow in a following correlation matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400c0e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cor(standarized_X_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9861c4f6",
   "metadata": {},
   "source": [
    "### Summary of Principal Component Analysis (PCA)\n",
    "- New representation in lower dimensional space (e.g. less than 10 or 2 for visuall purposes) is achieved via linear combination of original features (usually more than 100) and loadings\n",
    "- PCA exploits the correlation between features. The would be no sens and no value-added of using PCA, if variables were orthoghonal, i.e. idealy independent.\n",
    "- PCA is equivalent to deep learning autoencoder with linear activation functions\n",
    "- PCA requires data standarization, as its ommision will results in relative focus on variables with higher variance\n",
    "- even though PCA results of Principal Components don't have direct interpretation, they can be thought as indices being composed of original features and being able to represent high dimenstional (often >100) dataset by less than 10 Principal Components "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c9070e",
   "metadata": {},
   "source": [
    "## Uniform Manifold Approximation & Projection (UMAP)\n",
    "Now let's try a modern dimensionality reduction algorithm called **UMAP** (Uniform Manifold Approximation & Projection). It is rooted in Riemannian geometry - details can be found in the [paper](https://arxiv.org/abs/1802.03426). UMAP proved to give really good results and is considered state-of-the-art."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d43860",
   "metadata": {},
   "source": [
    "Function `umap` returns a new data representation into 2-dimentional embedding. An important input parameter is `n_neighbors`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5bff18",
   "metadata": {},
   "outputs": [],
   "source": [
    "UMAP_embedding = umap(standarized_X_matrix', n_neighbors = 10)'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1d8fc4",
   "metadata": {},
   "source": [
    "As previously, we set up a vector composed of 50 states describing its embedding coordinates and graphical parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c6e91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "UStates_visualization = [ (UMAP_embedding[i,1], UMAP_embedding[i,2],\n",
    "                        text(data.State[i], 6, :blue)) for i=1:obs_number ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2657e1",
   "metadata": {},
   "source": [
    "Finally, we draw a two-dimensional scatterplot with our observations of US states into embedded space: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaff3e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PC_limit = ceil(Int, maximum(PrincipalComponents))\n",
    "plot( ; annotations= UStates_visualization , legend = false,\n",
    "    xlim=[floor(Int, minimum(UMAP_embedding[:,1])), ceil(Int, maximum(UMAP_embedding[:,1]))],\n",
    "    ylim=[floor(Int, minimum(UMAP_embedding[:,2])), ceil(Int, maximum(UMAP_embedding[:,2]))],\n",
    "    xlabel = \"1st dimension of UMAP embedding\",\n",
    "    ylabel = \"2nd dimension of UMAP embedding\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee635ead",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "### k-means algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444b8e01",
   "metadata": {},
   "source": [
    "K-means algorithm in the nutshell:\n",
    "1. Pick randomly 'k' observations from the dataset - initial centroids\n",
    "2. Assign other observations to the nearest centroid\n",
    "3. Calculate average coordinates from the members of the clusters - new coordinates of the center\n",
    "4. Repeat 2. and 3. until stop criterion is reached\n",
    "\n",
    "We start with a simple and simulated dataset demonstrating the potential Data Generating Process for k-mean algorithm and how it can be applied to this dataset. We start with defining two basic data generation parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ccec9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_per_cluster = 100\n",
    "true_centroids = [(2, 2), (6, 6), (2,6), (6,2)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceeb2fef",
   "metadata": {},
   "source": [
    "Data Generating Process (DGP):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f729ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = (true_centroids[1] .+ randn(2,n_per_cluster))'\n",
    "for centroid_number in 2:length(true_centroids)\n",
    "    observations = vcat(observations, (true_centroids[centroid_number] .+ randn(2,n_per_cluster))')\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e6edea",
   "metadata": {},
   "source": [
    "Generated data visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9eb6ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(observations[:,1], observations[:,2], color=repeat(1:length(true_centroids),\n",
    "        inner=n_per_cluster), xlab=\"X1\", ylab=\"X2\", legend=false, title=\"Simulated clustered data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b02a55c",
   "metadata": {},
   "source": [
    "Fitting k-means algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ae1a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_kmeans=kmeans(observations, 4, nstart = 100, maxiter = 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef41138e",
   "metadata": {},
   "source": [
    "Visualization of k-means clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35c73f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(observations[:,1], observations[:,2], color=fitted_kmeans.cluster,\n",
    "        xlabel=\"X1\", ylab=\"X2\", legend = false, title=\"Data clustered by k-means\")\n",
    "estimated_centroids = reduce(hcat, fitted_kmeans.centroids)\n",
    "scatter!(estimated_centroids[1,:], estimated_centroids[2,:],markershape=:xcross,\n",
    "        markersize = 10, markercolor=:black, msw=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d70d63",
   "metadata": {},
   "source": [
    "Generating an artificial dense prediction set to create a classification regeions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc5ca91",
   "metadata": {},
   "outputs": [],
   "source": [
    "denseset_n=10000\n",
    "denseset=[8*rand(denseset_n) rand(denseset_n)*8]\n",
    "predicted_cluster=[argmin(sum((denseset[i,:] .- estimated_centroids) .^2, dims = 1))[2] for i in 1:denseset_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f86ef4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(denseset[:,1], denseset[:,2], color=predicted_cluster, xlabel=\"X1\", ylab=\"X2\",\n",
    "        markersize=6, msw=0,title=\"Classification regions\", legend=false)\n",
    "scatter!(observations[:,1], observations[:,2],\n",
    "        color=:black, xlabel=\"X1\", ylab=\"X2\", markersize=2)\n",
    "scatter!(estimated_centroids[1,:], estimated_centroids[2,:],markershape=:xcross,\n",
    "        markersize = 10, markercolor=:white, msw=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bff8b5c",
   "metadata": {},
   "source": [
    "Then we move to a real-world dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedcd077",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_kmeans=kmeans(standarized_X_matrix, 4, nstart = 100, maxiter = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a7c2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(PrincipalComponents'[:,1], PrincipalComponents'[:,2], color=fitted_kmeans.cluster, legend = false,\n",
    "xlabel = \"1st Principal Component\",\n",
    "ylabel = \"2nd Principal Component\",\n",
    "title = \"K-means clustering in PCA embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a685f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(UMAP_embedding[:,1], UMAP_embedding[:,2], color=fitted_kmeans.cluster, legend = false,\n",
    "xlabel = \"1st dimension of UMAP embedding\",\n",
    "ylabel = \"2nd dimension of UMAP embedding\",\n",
    "title = \"k-means clustering in UMAP embedding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe92f688",
   "metadata": {},
   "source": [
    "**Elbow** method to pick k - inertia for given k-means clustering is the sum of squares between clusters' center and their members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c132b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_number_of_clusters = 20\n",
    "within_sum_squares=[kmeans(standarized_X_matrix, k, nstart=500, maxiter=100).withinss\n",
    "                    for k in 1:max_number_of_clusters]\n",
    "plot(1:max_number_of_clusters, within_sum_squares,\n",
    "    ylim=(0,within_sum_squares[1]), legend = false,\n",
    "    xlabel = \"Number of clusters\",\n",
    "    ylabel = \"Within sum of squares\",\n",
    "    title = \"Elbow plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7adfa6",
   "metadata": {},
   "source": [
    "### Summary of k-means algorithm\n",
    "- k-means might a support segmentation exercise, but resulted segments will heavily depend on the subjective selection of input variables and number of segments K. These should be specified rather by a subject matter expert rather than a data scientist. Segmentation is more an art than a science and therfore rather not so easy task.\n",
    "- requires data standarization, since no standarization will result in clusters defined accros variables with high variables\n",
    "- each run of k-means algorithm starts with a randomized starting points, i.e. cluster centroids, so the results might differ from run to run\n",
    "- since k-means is not global optimization method, it might identify local solution, so it's important to allow an algorithm to be rerun multiple times with different starting points, i.e. cluster centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c001054",
   "metadata": {},
   "source": [
    "*Preparation of this workshop has been supported by the Polish National Agency for Academic Exchange under the Strategic Partnerships programme, grant number BPI/PST/2021/1/00069/U/00001.*\n",
    "\n",
    "![SGH & NAWA](../logo.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.2",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "4bc6cd0889c762a07a1702587a64f149c2344a870092f24d4cf069101d324d1d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
