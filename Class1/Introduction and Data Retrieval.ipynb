{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d39be91b",
   "metadata": {},
   "source": [
    "# Introduction to ML Workflow and Data Retrieval in Julia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8498433",
   "metadata": {},
   "source": [
    "## Setting up the project (environment) in Julia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9ba1ec",
   "metadata": {},
   "source": [
    "Before proceeding to topics related to machine learning and data analysis, let's begin with general software engineering idea - **environments**. It's a best practice to prepare isolated project for every programming initiative, whether it is building a web application, running complex scientific simulation or training ML model.\n",
    "\n",
    "The idea of structured project is prevalent in all programming languages, however the actual name may differ, e.g. [virtual environment](https://docs.python.org/3/library/venv.html) in Python or [build environment](https://docs.gradle.org/current/userguide/build_environment.html) in many compiled languages. The scope of information carried by the projects differ between programming languages as well, but in general they record:\n",
    "* dependencies (libraries, packages) required by the project\n",
    "* version of the project (see [semantic versioning](https://semver.org/))\n",
    "* indication of development stage (development, test, production, etc.)\n",
    "* author-related information (name, contact information, company affiliation)\n",
    "* miscellaneous configurations (compiler flags, version control details, CI/CD parameters, IDE settings)\n",
    "\n",
    "The main goals of the project are to:\n",
    "* provide reproducibility and standardization (if it works on my machine, it should work on yours), \n",
    "* enable collaboration (shared projects within teams)\n",
    "* supply additional information about the piece of software (date of creation, sponsoring company name, author's email)\n",
    "\n",
    "In Julia the project is defined by two files: `Project.toml` and `Manifest.toml`. More information on both can be found in [Pkg.jl documentation](https://pkgdocs.julialang.org/v1/toml-files/).\n",
    "\n",
    "For example, below extract from DataFrames.jl `Project.toml` contains name of the package, it's unique identifier, current version and one dependency on `DataAPI` package with additional version restriction:\n",
    "\n",
    "```julia\n",
    "name = \"DataFrames\"\n",
    "uuid = \"a93c6f00-e57d-5684-b7b6-d8193f3e46c0\"\n",
    "version = \"1.4.1\"\n",
    "\n",
    "[deps]\n",
    "DataAPI = \"9a962f9c-6df0-11e9-0e5d-c546b8b5ee8a\"\n",
    "(...)\n",
    "\n",
    "[compat]\n",
    "DataAPI = \"1.12.0\"\n",
    "(...)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c689b93",
   "metadata": {},
   "source": [
    "You can start your own Project by running **any of the two** cells below. More information on project initiation is available again in [Pkg.jl documentation](https://pkgdocs.julialang.org/v1/environments/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7361b0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982e3700",
   "metadata": {},
   "outputs": [],
   "source": [
    "]activate ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370c37b2",
   "metadata": {},
   "source": [
    "Add all required packages (dependencies) to the project using `]add` command. After finishing the work you can share the notebook (and corresponding TOML files). With high degree of confidence, recipients will be able to run the notebook without interference and errors related to compatibility or missing libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47285c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "]add DataFrames, JSON, Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea62b272",
   "metadata": {},
   "source": [
    "To replicate the environment, run `]instantiate` command in the folder where `Project.toml` is present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082616a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "]instantiate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8437adb4",
   "metadata": {},
   "source": [
    "Having said that, **make sure to run `]instantiate` command above**, so that TOML files provided with the notebook will be used to install all required packages with expected versions to make your experience in upcoming exercises smooth and pleasant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133be736",
   "metadata": {},
   "source": [
    "## Machine Learning workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af1eb8b",
   "metadata": {},
   "source": [
    "![](Class1_ML_Workflow.png)\n",
    "<div style=\"text-align: right\">Source: Burkov Andriy, ML Engineering, 2020, CC BY-SA 4.0</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf7cfe3",
   "metadata": {},
   "source": [
    "Lifecycle of Machine Learning project is a complex process involving multiple areas of expertise and set of skills. To succeed with an enterprise-level ML project, we'll need:\n",
    "* project managers, product owners and business analysts with good understanding of business problem and ability to define the goal and manage the execution of the initiative\n",
    "* data engineers, data analysts and data scientists with in-depth knowledge about the data, technical skills and statistical (modelling) expertise\n",
    "* DevOps engineers, software engineers, application developers taking care of model deployment in secure, robust and performant manner, often with embedment into a bigger application\n",
    "\n",
    "Increasing penetration of ML models in the business and advancements in the deployment areas created a new area of **Machine Learning engineering** and corresponding position of **Machine Learning engineer**. Activities related to post-evaluation steps are also often reffered to as **MLOps (Machine Learning Operations)**, similarly to operations term used for ongoing maintanance and monitoring of software applications.\n",
    "\n",
    "___\n",
    "We'll cover several elements of the process in the notebooks throught the course, focusing mainly on the steps from Data collection up to Model evaluation. The final notebooks will cover basics of the deployment and serving topics.\n",
    "\n",
    "Let's start with the first step in the ML journey - obtaining and loading data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8426429",
   "metadata": {},
   "source": [
    "## Data Gathering  and Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfe3002",
   "metadata": {},
   "source": [
    "Build a machine learning model is a data-heavy exercise and often the success of the whole project may be determined by quality and quantity of data. As for the former, the famous saying 'garbage-in, garbage-out' says it all - data may be cleaned and preprocessed, but there is hardly anything to be done if the inherent information carried by the dataset is weak (e.g. non-related features) or scarce (e.g. many missing values).\n",
    "\n",
    "There is abandunace of data sources and formats eligible for usage in ML process. Data can be categorized based on the presence (or lack) of structure into:\n",
    "* **structured data** (fixed schema, often identified with tabular datasets)\n",
    "* **semi-structured data** (varying schema between observations, usually stored in JSON or XML format)\n",
    "* **unstructured data** (domain-specific data without exposed features, e.g. text, music, images)\n",
    "\n",
    "Affiliation with one of the categories above translates into available ML tasks, e.g. regression and classification is usually considered for structured data, object detection is task specific to computer vision, while sentiment analysis will be applied on text data containing natural language.\n",
    "\n",
    "___\n",
    "Another factor to consider when thinking about the data is it's source. Majority of small scale or Proof-of-Concept projects are based on the **flat files** available in online repositories - either ones specialized for modelling purposes (e.g. [UCI](https://archive.ics.uci.edu/ml/index.php)) or more general like GitHub. Government agencies also tend to share cyclic reports online in flat files (e.g. [OECD](https://data.oecd.org/)).\n",
    "\n",
    "For medium and large ML initiatives more suitable source of data would be a **database**, **data warehouse** or **data lake**. Relational databases usually deliver structured data, however modern solutions support semi-structured and unstructured data as well (see [JSON support in PostgreSQL](https://www.postgresql.org/docs/current/datatype-json.html) for example). For big volumes of historical structured data a data warehouse is a common solution, while data lake support all three structure categories. \n",
    "\n",
    "Data source commonly connected with semi-structured data is a **REST API**. It's a service that can be queried through exposed URL endpoints and typically response with data in JSON format. Received records are then parsed and turned into structured data or used directly in ML models in a semi-structured form.\n",
    "\n",
    "Public cloud platforms can also serve as  a modern data source for modelling. **Object storages** such as AWS S3 or Azure Blob Storage provide a convenient place to store and share large quantities of unstructured data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5dec26",
   "metadata": {},
   "source": [
    "After the short overview of sources of data and categories of data structure, let's see how we can load various datasets into Julia for further processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5473ed2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "using DataFrames: DataFrame\n",
    "using Plots\n",
    "using StatsPlots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebd42ee",
   "metadata": {},
   "source": [
    "## Flat files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7787b819",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Downloads: download\n",
    "using CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081f0047",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are downloading the famous Iris dataset from UCI repository and the file with dataset description\n",
    "if !isfile(\"iris_data.csv\")\n",
    "    download(\"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\",\"iris_data.csv\");\n",
    "    download(\"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.names\",\"iris_names.txt\");\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63c4479",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flat files are present in the workind directory\n",
    "readdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005323d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we can load the data to DataFrame for analysis\n",
    "CSV.read(\"iris_data.csv\",DataFrame; header=[\"sepal_len\",\"sepal_wid\",\"petal_len\",\"petal_wid\",\"species\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9881a244",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Or without saving a file\n",
    "iris = CSV.read(download(\"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"), DataFrame;\n",
    "        header=[\"sepal_len\",\"sepal_wid\",\"petal_len\",\"petal_wid\",\"species\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec08e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we can use the dataset to produce a meaningful plot or build a model\n",
    "@df iris scatter(:petal_len, :petal_wid, group=:species, legend=:topleft, xlab=\"Petal length\", ylab=\"Petal width\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9a0831",
   "metadata": {},
   "source": [
    "## Interacting with API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d478a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "using JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2a21b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rpad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915dd86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Auxilary function to print a DataFrame schema\n",
    "function print_schema(df::DataFrame)\n",
    "    for (key, val) in Dict(zip(names(df),eltype.(eachcol(df))))\n",
    "        println(rpad(key,30,\" \"),\": \",val)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e965d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's query GitHub API for list of Julia-based repositories ranked by number of stargazers\n",
    "#It's a public API, so we don't neet to pass any authentication information\n",
    "download(\"https://api.github.com/search/repositories?q=language:julia&sort=stars\",\"gh_api.json\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5425459",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First 300 characters of obtained JSON file\n",
    "#JSON contains `items` array with objects representing repositories\n",
    "#There are also nested objects in repositories, e.g. `owner`\n",
    "julia_repos = read(\"gh_api.json\", String);\n",
    "print(julia_repos[begin:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd83db9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#JSON can also fit to a DatFrame - nested objects are loaded as Dict columns and arrays as Array\n",
    "repos_df = DataFrame(JSON.parse(julia_repos)[\"items\"]);\n",
    "first(repos_df,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192a67d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#After inspecting the schema we can determine which fields are of composite type\n",
    "print_schema(repos_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dabd43",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Information about top 10 repositories ordered by stars count\n",
    "for repo in eachrow(first(repos_df,10))\n",
    "    println(\"\"\"Name: $(repo[\"name\"])\n",
    "    Owner: $(repo[\"owner\"][\"login\"])\n",
    "    Stars: $(repo[\"stargazers_count\"])\n",
    "    Created: $(repo[\"created_at\"])\n",
    "    Description: $(repo[\"description\"])\\n\"\"\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966a7cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Barplot based on semi-structured JSON data\n",
    "#`julia` itself is excluded due to high stars count compared to other repositories\n",
    "plotly()\n",
    "n=20\n",
    "@df first(repos_df[2:end,:],n) bar(1:n,:stargazers_count, hovertext=:description,\n",
    "    size = (900,500), xrotation=45, legend=false, xticks=(1:n, :name), ylab=\"Stars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b87195",
   "metadata": {},
   "source": [
    "## Unstructured data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a20577b",
   "metadata": {},
   "source": [
    "Data can also be obtained directly through packages. `MLDatasets.jl` allows us to conveniently load few well-known unstructured datasets, in particular from computer vision area. We'll load and inspect observations from [FashionMNIST](https://github.com/zalandoresearch/fashion-mnist) dataset which contains preprocessed images of clothes (28x28 pixels, single channel). You may need to accept a disclaimer when running the first function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff394450",
   "metadata": {},
   "outputs": [],
   "source": [
    "using MLDatasets\n",
    "using ImageCore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c55f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MLDatasets did part of job for us and parsed binary data into a matrix. \n",
    "#Each cell contains value between 0.0 - 1.0 corresponding to the 'brightness' of the pixel in an image.\n",
    "#There is no explicit structure in a form of named columns of specific type, hence the dat is unstructured.\n",
    "FashionMNIST.testtensor(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc9af53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the sake of visualisation, we can interpet 0.0 as black and 1.0 as white\n",
    "#MLDatasets provide an auxilary function `convert2image` to produce a nice plot\n",
    "#Looks like the first test observation is a shoe\n",
    "FashionMNIST.convert2image(FashionMNIST.testtensor(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93e4c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Colors can be swapped easily with broadcasting\n",
    "#Second sample is apparently a sweater with an imprint\n",
    "FashionMNIST.convert2image(1 .- FashionMNIST.testtensor(2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.2",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
